{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is classification of mnist data through lstm model\n",
    "# this is just for the sake of understanding lstm better\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is into train, test, and validation\n",
    "# each image is hving a label for digit 0-9, but here we r taking label in one hot encoding\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting hyper parameters for our classifier model\n",
    "lr = 0.001\n",
    "batchSize = 128\n",
    "epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here in the model i am using 2 layers and one lstm cell structure.\n",
    "# the 1st hidden layer is input layer, then lstm cell, and then hidden output layer \n",
    "\n",
    "#here as we r using self made hidden layer for input before lstm cell, so we have to\n",
    "# give them weights, biases and manipulate input dimensions n features as we want to feed lstm with\n",
    "\n",
    "# We know that Lstm (rnn) processes sequential data, so to fit the mnist image data in lstm\n",
    "# the mnist image is a matrix of 28*28, hence to feed in lstm, we consider this image as a \n",
    "# 28 rows where each row is having 28 units or points in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# giving parameters for layers\n",
    "columns = 28\n",
    "# this represents the no. of colun in mnist image\n",
    "timeSteps = 28\n",
    "# the rows that we aconsidering can be thought of as time steps in lstm's\n",
    "\n",
    "hiddenUnits = 128\n",
    "# this is the no. of neurons that will be in the hidden layer\n",
    "\n",
    "classes = 10\n",
    "# as we classify it into 0-9 (10 classses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making placeholders for lstm network\n",
    "# here the placeholder can be considered as 3 dimensional\n",
    "# 1st dimensionn as usual none to accept the batch size,\n",
    "# 2nd dimension gives how many rows or time steps\n",
    "# the 2nd dimension represents timestep, so the no of time steps = no. of lstm cells\n",
    "#3rd dimension gives how many points (analogous to words) in each row\n",
    "\n",
    "# in output y placeholder, we are just having the output class from 0-9 in\n",
    "# one hot encoding\n",
    "# 1st dimension usual none, for batch size\n",
    "# 2nd dimension 10 for one hot encoding\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, timeSteps, columns])\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now for making weights and biases of input layer\n",
    "# we know that input layer is hving 128 neurons, and the no of input features to these neurons \n",
    "# will be the no. of columns image is having that is 28\n",
    "\n",
    "inputLayerWeights = tf.Variable(tf.random_normal([columns, hiddenUnits]))\n",
    "# as biases need not be updated we can keep that in constant\n",
    "inputLayerBiases = tf.constant(0.1, shape = [hiddenUnits, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output layer weights n biases\n",
    "outputLayerWeights = tf.Variable(tf.random_normal([hiddenUnits, classes]))\n",
    "#as output layer has 128 neurons and output into 10 classes\n",
    "outputLayerBiases = tf.constant(0.1, shape = ([classes, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we will define fn to compile all layers together including BasicLstm\n",
    "def myLstmNetwork(inputs, weights, bias):\n",
    "    \n",
    "    #to prepare input layer we have to reshape the data coming from input placeholder\n",
    "    #coz weights are in diff format \n",
    "    # X ==> (128 batch * 28 steps, 28 inputs)\n",
    "    # -1 is used in reshape to either flatten or infer shape, look doc for more\n",
    "    X = tf.reshape(inputs, [-1, columns])\n",
    "    # the column will remain the same shape as it is feature\n",
    "    Xin = tf.matmul(X, weights) + bias\n",
    "    # Xin will b the output of input layer from the 128 hidden neurons\n",
    "    # X_in ==> (128 batch * 28 steps, 128 hidden)\n",
    "    \n",
    "    # now we again reshape it into batches and make it 3d\n",
    "    \n",
    "    Xout = tf.reshape(Xin, [-1, timeSteps, columns])\n",
    "    \n",
    "    # now we will make lstm cell\n",
    "    \n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hiddenUnits, forget_bias = 1.0, state_is_tuple = True)\n",
    "    \n",
    "    # lstm cell is divided into two parts (c_state, h_state)\n",
    "    # look for cell state in tensorflow\n",
    "    initState = cell.zero_state(batchSize, dtype=tf.float32)\n",
    "    # we have initialized the cell with first zero state with batch size\n",
    "    \n",
    "    #now we will built our complete rnn with these cells \n",
    "    # we will use dynamic rnn than rnn, in rnn it creates static length rnn, say if we create net having 100 \n",
    "    #timesteps it will  fixed.(slower also)\n",
    "    # using dynamic rnn it dynamically calculates the time steps with its while loop\n",
    "    \n",
    "    ouputs,finalState = tf.nn.dynamic_rnn(cell, Xout, initial_state = initState, time_major = False)\n",
    "    \n",
    "    # here our time_major false means the first dimension of the input is not time step. here it is \n",
    "    # batch size\n",
    "    #final state is the ouput from the final cell i guess\n",
    "    \n",
    "    #finalState will b returned in tuple format, (cstate, hstate)\n",
    "    #we r interested only in final state coz this is a memory lstm\n",
    "    \n",
    "    results = tf.matmul(finalState[1], outputLayerWeights) + outputLayerBiases\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = myLstmNetwork(x, inputLayerWeights, inputLayerBiases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculating accuracy\n",
    "\n",
    "corrPrediction = tf.equal(tf.argmax(prediction, axis = 1), tf.argmax(y, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(corrPrediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
